{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ee60cfcf","cell_type":"markdown","source":"# **Clustering**","metadata":{}},{"id":"0e969a61","cell_type":"markdown","source":"# Iris Dataset\n\nIl dataset Iris è un classico dataset nell'apprendimento automatico e nella statistica, introdotto da Ronald Fisher nel 1936. È comunemente utilizzato per attività di classificazione e clustering.\n\n## Caratteristiche e Struttura\n- **Campioni**: 150 campioni di fiori iris.\n- **Features**:\n  - Lunghezza del sepalo (cm)\n  - Larghezza del sepalo (cm)\n  - Lunghezza del petalo (cm)\n  - Larghezza del petalo (cm)\n- **Classi (Etichette Target)**:\n  - *Iris-setosa*\n  - *Iris-versicolor*\n  - *Iris-virginica*\n\nOgni classe è rappresentata da 50 campioni.\n\n## Caratteristiche Principali\n- **Balanced Dataset**: Ogni classe contiene lo stesso numero di campioni.\n- **Perfect for Beginners**: a sua semplicità e struttura ben definita lo rendono perfetto per scopi didattici.\n- **Separable Classes**:\n  - *Iris-setosa* è linearmente separabile dalle altre due classi.\n  - *Iris-versicolor* e *Iris-virginica* sono più difficili da separare tra loro.\n\n\n# Iris Dataset Classes\n\n<table>\n    <tr>\n        <th>Iris Setosa</th>\n        <th>Iris Versicolor</th>\n        <th>Iris Virginica</th>\n    </tr>\n    <tr>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSFn-u9Lagrv8pV4zJ8Z1cEqXNL_uo39CrL6A&s\" alt=\"Iris setosa\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://www.waternursery.it/document/img_prodotti/616/1646318149.jpeg\" alt=\"Iris versicolor\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQbTwTLA7_7SeTE3B1QOKw0TlB8Rp6NU7vyg&s\" alt=\"Iris virginica\" width=\"300\" height=\"300\"></td>\n    </tr>\n</table>\n\n","metadata":{}},{"id":"e83ad2f0","cell_type":"markdown","source":"# **K-means (matematica)**","metadata":{}},{"id":"7a6201e6","cell_type":"markdown","source":"### NumPy: `np.linalg.norm`\n`np.linalg.norm` calcola la norma di un vettore, matrice o array. Viene spesso utilizzato per calcolare distanze o grandezze.\n\n**Sintassi**:\n```python\nnp.linalg.norm(array, axis=None, ord=None)\n```\n\n**Parametri**:\n- `array`: Array di input per cui viene calcolata la norma.\n- `axis`: Specifica l'asse lungo cui calcolare la norma. Se `None`, calcola la norma dell'intero array.\n- `ord`: Definisce il tipo di norma (es. 2 per norma Euclidea, 1 per norma Manhattan).\n\n**Esempio**:\n```python\nimport numpy as np\nvector = np.array([3, 4])\neuclidean_norm = np.linalg.norm(vector)\nprint(euclidean_norm)  # Output: 5.0 (distanza euclidea)\n```\n\n---\n\n### NumPy: `np.allclose`\n`np.allclose` verifica se due array sono uguali elemento per elemento entro una certa tolleranza.\n\n**Sintassi**:\n```python\nnp.allclose(array1, array2, rtol=1e-05, atol=1e-08)\n```\n\n**Parametri**:\n- `array1`, `array2`: Array da confrontare.\n- `rtol`: Tolleranza relativa.\n- `atol`: Tolleranza assoluta.\n\n**Ritorna**:\n- `True` se tutti gli elementi sono entro la tolleranza, altrimenti `False`.\n\n**Example**:\n```python\nimport numpy as np\narray1 = np.array([1.0, 2.0, 3.0])\narray2 = np.array([1.0, 2.001, 3.0])\nis_close = np.allclose(array1, array2, atol=0.01)\nprint(is_close)  # Output: True","metadata":{}},{"id":"2b12be7d","cell_type":"markdown","source":"## **Esercizio 1: Implementare algoritmo K-means**\n\nNel primo esercizio vi è richiesto di implementare l' algoritmo K-means. Di seguito troviamo una guida degli step da seguire:\n\n**1.** Loading dei dati e standardizzazione.\n\n**2.** Applicare PCA per ridurre a 2 dimensioni.\n\n**3.** Impostare parametri per K-means, cioè numero k di clusters, massimo numero di iterazioni e un seed.\n\n**4.** Inizializzazione dei centroidi.\n\n**5.** Iterare l' algoritmo.\n\n### **Algoritmo K-means**\n\nPer iterare l' algoritmo k-means ricordiamo che esso segue degli step ben precisi:\n\n**1.** **Assegnazone dei punti ai clusters**, calcolando la distanza tra ogni punto e i centroidi. Un punto verrà assegnato al centroide più vicino.\n\n**2.** **Aggiornamento centroidi.** Modifico la posizione dei centroidi con la media dei punti che sono stati assegnati a quel cluster.\n\n**3.** **Verifica convergenza**, misurando quanto distano i nuovi centroidi da quelli vecchi. Se questa differenza soddisfa una certa soglia (tolleranza) allora siamo giunti a convergenza. N.B. Se la convergenza non è raggiunta si continuerà ad iterare. Per continuare ad iterare devo però sostituire i nuovi centroidi.\n\n","metadata":{}},{"id":"af0ad3c4","cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# 1. PREPARAZIONE DATI\n# Carica il dataset Iris\niris = load_iris()\nX = iris.data    # Matrice dei dati: 150 fiori × 4 caratteristiche\ny = iris.target  # Etichette vere (non usate nel clustering)\n\n# Standardizza i dati per dare uguale peso a tutte le caratteristiche\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Applica PCA per ridurre a 2 dimensioni\n\npca = PCA(n_components = 2)\nX_pca = pca.fit_transform(X_scaled) ","metadata":{},"outputs":[],"execution_count":null},{"id":"7aec2d4e","cell_type":"code","source":"# 2. IMPOSTAZIONE PARAMETRI\n\nnp.random.seed(42)  # Per riproducibilità\n#Ogni volta che faccio partire il programma ho sempre \n#gli stessi numeri casuali\nk = 3               # Numero di cluster desiderati\nmax_iters = 20     # Numero massimo di iterazioni","metadata":{},"outputs":[],"execution_count":null},{"id":"4901a78a","cell_type":"code","source":"# 3. INIZIALIZZAZIONE CENTROIDI\n# Seleziona k punti casuali dal dataset come centroidi iniziali\ncentroids_indices = np.random.choice(X_pca.shape[0], size=k, replace=False)\ncentroids = X_pca[centroids_indices]","metadata":{},"outputs":[],"execution_count":null},{"id":"b65d7612","cell_type":"code","source":"# 4. ALGORITMO K-MEANS\nfor iterazione in range(max_iters):\n    # FASE 1: Assegnazione punti ai cluster\n    # Calcola la distanza di ogni punto da ogni centroide\n    distances = np.linalg.norm(X_pca[:, np.newaxis]-centroids, axis=2)\n    #Trasformo l'array da shape (n,d) a (n,1,d) per effettuare broadcasting\n    \n    # Assegna ogni punto al centroide più vicino\n    labels = np.argmin(distanze, axis=1)\n    # FASE 2: Aggiornamento centroidi\n    # Calcola i nuovi centroidi come la media dei punti assegnati a ciascun cluster   \n    new_centroids = np.array([X_pca[labels == i].mean(axis=0) for i in range(k)])\n    \n    # FASE 3: Verifica convergenza\nif np.allclose(centroids, new_centroids, atol=1e-4):\n        print(f\"Convergenza raggiunta all'iterazione {iterazione}\")\n        break\n    # svolgimento...\n    \n    # Aggiorna i centroidi per la prossima iterazione\n    \n    centroids= new_centroids","metadata":{},"outputs":[],"execution_count":null},{"id":"94fadffd","cell_type":"markdown","source":"### **Visualizzazione**\n\nUtilizzate la seguente funzione per visualizzare il risultato dell' algoritmo. La funzione richiede 3 parametri:\n\n**1.** I dati su cui è stato applicato l' algoritmo di clustering.\n\n**2.** Le labels all' ultima iterazione dell' algoritmo.\n\n**3.** I centroidi all' ultima iterazione dell' aloritmo.","metadata":{}},{"id":"0a4b7ce5","cell_type":"code","source":"def visualizza_clusters(X, labels, centroids):\n    \"\"\"\n    Visualizza i risultati del clustering.\n    \n    Parametri:\n    X: array dei dati (standardizzati)\n    labels: array delle etichette dei cluster\n    centroids: array dei centroidi\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    \n    colori = ['red', 'green', 'blue']\n    \n    for i in range(len(np.unique(labels))):\n        mask = labels == i\n        plt.scatter(X[mask, 0], X[mask, 1],\n                   c=colori[i], \n                   label=f'Cluster {i}',\n                   alpha=0.6)\n    \n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                c='black',\n                marker='x',\n                s=200,\n                label='Centroidi')\n    \n    plt.xlabel(\"Prima caratteristica\")\n    plt.ylabel(\"Seconda caratteristica\")\n    plt.title(\"Risultati K-means\")\n    plt.legend()\n    plt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"id":"7d5bed57","cell_type":"code","source":"# Chiamare la funzione visualizza clusters\n\nvisualizza_clusters(X_pca, labels, centroids)","metadata":{},"outputs":[],"execution_count":null},{"id":"ab9f3087","cell_type":"markdown","source":"## **Esercizio 2: Valutare l' algoritmo di clustering**\n\nScriviamo una funzione che utilizzi diverse metriche per valutare l' algoritmo di clustering. ATTENZIONE: questa funzione potrà essere riutilizzata più avanti anche per altri algoritmi.\n\nLe metriche da implementare sono le seguenti:\n\n* **Silhouette score**\n\n* **Davies Bouldin score**\n\n* **Rand Index**\n\n* **Adjusted Rand Index**\n\nPer implementarle utilizzeremo direttamente le loro implementazioni fornite da `sklearn`.\n\n## **Sintassi**\n\n```python\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nsilhouette = silhouette_score(X, labels)\ndbs = davies_bouldin_score(X, labels) # entrambe le metriche richiedono i dati e le labels\nprint(silhouette)\nprint(dbs)\n```\n\n```python\nfrom sklearn.metrics import rand_score, adjusted_rand_score\nrand = rand_score(y, labels)\nadj_rand = adjusted_rand_score(y, labels) # Queste due metriche invece richiedono le lables originali e quelle assegnate dall' algoritmo\nprint(rand)\nprint(adj_rand)\n```","metadata":{}},{"id":"f5da4284","cell_type":"code","source":"from sklearn.metrics import silhouette_score, davies_bouldin_score, rand_score, adjusted_rand_score\n\n# VALUTAZIONE DEL CLUSTERING\ndef evaluate_clustering(labels, X, y):\n\n    silhouette = silhouette_score(X, labels)\n\n    dbi = davies_bouldin_score(X, labels)\n\n    rand = rand_score(y, labels)\n\n    adj_rand = adjusted_rand_score(y, labels)\n    \n    return silhouette, dbi, rand, adj_rand\n","metadata":{},"outputs":[],"execution_count":null},{"id":"b0c2759f","cell_type":"code","source":"silhouette, dbi, rand, adj_rand = evaluate_clustering(labels, X, y)\n\nprint(f\"Silhouette Score: {silhouette:.4f}\")\nprint(f\"Davies-Bouldin Index: {dbi:.4f}\")\nprint(f\"Rand Index: {rand:.4f}\")\nprint(f\"Adjusted Rand Index: {adj_rand:.4f}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"5248cf5b","cell_type":"markdown","source":"## **Esercizio 3: Clustering al variare dei parametri**\n\nVediamo adesso come variano le prestazioni del clustering al variare di due parametri: il numero di clusters e il numero di componenti di PCA.\n\nPer semplicità utilizzeremo l' implementazione di `sklearn` per l' algoritmo K-means.\n\nAlla fine bisognerà stampare la configurazione di parametri che ci fa ottenere le migliori prestazioni utilizzando l' Adjusted Rand Index come metrica.\n\n### **Sintassi**:\n```python\nkmeans = KMeans(n_clusters=n)\nlabels = kmeans.fit_predict(data)\n```","metadata":{}},{"id":"2ce1d363","cell_type":"code","source":"from sklearn.cluster import KMeans\n\nn_pca_components = [2,3,4]\nn_clusters = [2,3,4]\n\nbest_ari = -1\n\nfor components in n_pca_components: \n    pca = PCA(n_components= n_pca_components)\n    X_pca = pca.fit_transform(X_scaled)\n    for clusters in n_clusters: \n        kmeans = KMeans(n_clusters=n, n_init=10)\n        labels = kmeans.fit_predict(X_pca)\n        adj_rand = adjusted_rand_score(y, labels)\n        if adj_rand > best_ari: \n            best_ari = adj_rand \n            best_config = (components, clusters)","metadata":{},"outputs":[],"execution_count":null},{"id":"382477a5","cell_type":"markdown","source":"## **Visualizzazione**\n\nLa seguente funzione di visualizzazione mostra la differenza tra i dati assegnati nei clusters e la loro effettiva divisione in classi.","metadata":{}},{"id":"fb34bd52","cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_clusters_vs_truth(X_2d, labels, y_true=None, cluster_centers=None, \n                          cluster_colors=None, class_colors=None, \n                          class_names=None, title_clusters=\"Risultati Clustering\", title_truth=\"Vere Classi\"):\n    \"\"\"\n    Visualizza affiancati (subplot 1x2) i dati clusterizzati e la distribuzione delle vere classi.\n    \n    Parametri obbligatori minimi:\n        - X_2d: array bidimensionale (come PCA) con shape (n_samples, 2)\n        - labels: array delle etichette di cluster per ciascun punto\n        - y_true: array delle etichette vere (opzionale; se non fornito mostra solo clustering)\n        - cluster_centers: array centroidi (opzionale; se fornito li visualizza)\n        - cluster_colors: lista di colori per i cluster (opzionale)\n        - class_colors: lista di colori per le classi vere (opzionale)\n        - class_names: lista di nomi per le classi vere (opzionale)\n        - title_clusters, title_truth: titoli dei plot (opzionali)\n    \"\"\"\n    n_clusters = len(np.unique(labels))\n    if cluster_colors is None:\n        cmap = plt.get_cmap(\"tab10\")\n        cluster_colors = [cmap(i) for i in range(n_clusters)]\n    if y_true is not None:\n        n_classes = len(np.unique(y_true))\n        if class_colors is None:\n            cmap2 = plt.get_cmap(\"tab10\")\n            class_colors = [cmap2(i) for i in range(n_classes)]\n    \n    fig, axs = plt.subplots(1, 2 if y_true is not None else 1, figsize=(12, 5) if y_true is not None else (6, 5))\n    axs = np.array(axs).reshape(-1)\n    \n    # Clustering plot\n    for i, cluster_label in enumerate(np.unique(labels)):\n        axs[0].scatter(X_2d[labels == cluster_label, 0], X_2d[labels == cluster_label, 1], \n                       color=cluster_colors[i], alpha=0.5, label=f\"Cluster {cluster_label}\")\n    if cluster_centers is not None:\n        axs[0].scatter(cluster_centers[:,0], cluster_centers[:,1], c='red', marker='x', s=100, label='Centroids')\n    axs[0].set_xlabel(\"Principal Component 1\")\n    axs[0].set_ylabel(\"Principal Component 2\")\n    axs[0].set_title(title_clusters)\n    axs[0].legend(loc='best')\n    axs[0].set_aspect('auto')\n    \n    # True class plot\n    if y_true is not None:\n        for i, class_label in enumerate(np.unique(y_true)):\n            label_name = class_names[class_label] if class_names is not None else f\"Class {class_label}\"\n            axs[1].scatter(X_2d[y_true == class_label, 0], X_2d[y_true == class_label, 1],\n                           color=class_colors[i], alpha=0.5, label=label_name)\n        axs[1].set_xlabel(\"Principal Component 1\")\n        axs[1].set_ylabel(\"Principal Component 2\")\n        axs[1].set_title(title_truth)\n        axs[1].legend(loc='best')\n        axs[1].set_aspect('auto')\n    plt.tight_layout()\n    plt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"id":"fa8c38fd","cell_type":"code","source":"# Utilizzare la funzione plot_clusters_vs_truth per visualizzare i risultati\n\nplot_clusters_vs_truth(X_pca, labels, y_true=y, cluster_centers=centroids, title_clusters=\"Risultati Clustering\", title_truth=\"Vere Classi\")","metadata":{},"outputs":[],"execution_count":null},{"id":"bd404869","cell_type":"markdown","source":"## **Esercizio 4: Gaussian Mixture Models**\n`GaussianMixture` modella i dati utilizzando una miscela di distribuzioni gaussiane. \nVogliamo valutare anche questo modello al variare di numero di componenti di PCA e numero di clusters.\n\n**Syntax**:\n```python\ngmm = GaussianMixture(n_components=n)\nlabels = gmm.fit_predict(data)\n```","metadata":{}},{"id":"c1c414d8","cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\nn_pca_components = [2,3,4]\nn_clusters = [2,3,4]\n\nfor pca_component in n_pca_components:\n    # Applicazione PCA per ridurre il numero di componenti\n    pca = PCA(n_components=pca_components)\n    X_pca = pca.fit_transform(X)  # X è il dataset originale\n    \n    for cluster in n_clusters:\n        # Creazione del modello GMM\n        gmm = GaussianMixture(n_components=n, random_state=42)\n        \n        # Applicazione del modello e predizione dei cluster\n        labels = gmm.fit_predict(X_pca)\n        \n        # Calcolo dell'Adjusted Rand Index\n        ari = adjusted_rand_score(y, labels)\n        \n        # Se troviamo un ARI migliore, aggiorniamo i parametri\n        if ari > best_ari:\n            best_ari = ari\n            best_n_pca = pca_component\n            best_n_clusters = cluster\n            \ngmm = GaussianMixture(n_components=best_n_clusters, random_state=42)\nlabels_gmm = gmm.fit_predict(X_pca)","metadata":{},"outputs":[],"execution_count":null},{"id":"84e5d417","cell_type":"code","source":"# visualizza i risultati del clustering GMM\nplot_clusters_vs_truth(X_2d=X_pca, labels=labels_gmm, y_true=y, cluster_centers=gmm.means_)","metadata":{},"outputs":[],"execution_count":null},{"id":"1753d524","cell_type":"markdown","source":"## **Esercizio 5: Agglomerative Clustering**\n\nAgglomerative Clustering utilizza un approccio gerarchico per raggruppare i punti dati. Il **linkage method** determina come viene calcolata la distanza tra i cluster quando vengono uniti.\n\n### Scikit-Learn: `AgglomerativeClustering`\n`AgglomerativeClustering` effettua clustering gerarchico con uno specifico metodo di **linkage**.\n\n**Syntax**:\n```python\nclustering = AgglomerativeClustering(n_clusters=n, linkage='method')\nlabels = clustering.fit_predict(data)\n```\n\nAnche in questo caso dovrete implementare l' algoritmo e valutarlo al variare del numero di clusters e numero di componenti della PCA.","metadata":{}},{"id":"258574cb","cell_type":"markdown","source":"### **Esercizio 5.1: Single Linkage**\n\nLa distanza tra due cluster è definita come la distanza minima tra qualsiasi coppia di punti appartenenti ai cluster (vicino più prossimo).\n\n#### Vantaggi\n\n* Rapido ed efficiente per dataset di grandi dimensioni.\n\n* Utile per rilevare cluster allungati o di forma irregolare.\n\n#### Svantaggi\n\n* Può causare effetti di \"concatenazione\", collegando i cluster in sequenza anziché formare gruppi compatti.\n\n#### Formula\n$$ d_{single}(C_1, C_2) = \\min_{x \\in C_1, y \\in C_2} ||x - y|| $$","metadata":{}},{"id":"0d3cec53","cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\niris = load_iris()\nX, y = iris.data, iris.target\n\nn_pca_components = [2, 3, 4]  \nn_clusters = [2, 3, 4]  \n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nbest_ari = -1 \nbest_pca = 0\nbest_clusters = 0\n\nfor pca_components in n_pca_components:\n    # Applicare PCA\n    pca = PCA(n_components=pca_components)\n    X_pca = pca.fit_transform(X)  \n    for clusters in n_clusters:\n        # AgglomerativeClustering\n        clustering = AgglomerativeClustering(n_clusters=clusters, linkage='single')\n        labels = clustering.fit_predict(X_pca)\n        \n        # Calcolare Adjusted Rand Index\n        ari = adjusted_rand_score(y, labels)\n        \n        # Memorizza il miglior ARI\n        if ari > best_ari:\n            best_ari = ari\n            best_pca = pca_components\n            best_clusters = clusters","metadata":{},"outputs":[],"execution_count":null},{"id":"3b5c4617","cell_type":"code","source":"# visualizzare i risultati del clustering gerarchico\n\nplot_clusters_vs_truth(X_pca, labels, y_true=y)","metadata":{},"outputs":[],"execution_count":null},{"id":"739571ab","cell_type":"markdown","source":"### **Esercizio 5.2: Complete Linkage**\nLa distanza tra due cluster è definita come la distanza massima tra qualsiasi coppia di punti appartenenti ai clusters (vicino più lontano).\n\n#### Vantaggi\n\n* **Cluster più compatti:** Produce gruppi densi e ben separati.\n\n* **Minore concatenamento:** Riduce l'effetto di \"catena\" tipico del Single Linkage.\n\n#### Svantaggi\n\n* **Sensibilità agli outlier:** Essendo basato sulle distanze massime, valori anomali influenzano fortemente i risultati.\n\n* **Dipendenza dalla scala:** Richiede una buona standardizzazione delle feature.\n\n\n#### Formula\n$$ d_{complete}(C_1, C_2) = \\max_{x \\in C_1, y \\in C_2} ||x - y|| $$","metadata":{}},{"id":"9c8dc246","cell_type":"code","source":"iris = load_iris()\nX, y = iris.data, iris.target\n\n\nn_pca_components = [2, 3, 4]  \nn_clusters = [2, 3, 4]  \n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\nbest_ari = -1  # Per tenere traccia del miglior ARI\nbest_n_pca = None\nbest_n_clusters = None\n\n# Svolgimento...\nfor n_pca in n_pca_components:\n    # Applica PCA\n    pca = PCA(n_components=n_pca)\n    X_pca = pca.fit_transform(X_scaled)\n    \n    for n_clusters in n_clusters_list:  # Usa un nome diverso per evitare conflitti\n        # Clustering con Agglomerative Clustering (Complete Linkage)\n        clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='complete')\n        labels = clustering.fit_predict(X_pca)\n        \n        # Calcola ARI\n        ari = adjusted_rand_score(y, labels)\n        \n        # Se il nuovo ARI è migliore, aggiorna la miglior configurazione\n        if ari > best_ari:\n            best_ari = ari\n            best_n_pca = n_pca\n            best_n_clusters = n_clusters\n","metadata":{},"outputs":[],"execution_count":null},{"id":"e94e50d5","cell_type":"code","source":"# visualizzare i risultati del clustering gerarchico\n\nplot_clusters_vs_truth(X_pca, labels, y_true=y, title_clusters=\"Clustering Gerarchico\", title_truth=\"Vere Classi\")","metadata":{},"outputs":[],"execution_count":null},{"id":"f6b6339f","cell_type":"markdown","source":"### **Esercizio 5.3: Average Linkage**\nLa distanza tra due cluster è definita come la distanza media tra tutte le coppie di punti appartenenti ai diversi cluster.\n\n#### Vantaggi\n\n* **Bilanciamento ottimale:** Mitiga l'impatto degli outlier e delle dimensioni dei cluster\n\n* **Cluster più naturali:** Produce raggruppamenti spesso più coerenti con la struttura reale dei dati.\n\n#### Svantaggi\n\n* **Costo computazionale elevato:** Richiede il calcolo di tutte le distanze pairwise, diventando proibitivo per dataset molto grandi.\n\n* **Sensibilità alla densità:** Può essere influenzato da cluster con densità disomogenee.\n\n\n#### Formula\n$$ d_{average}(C_1, C_2) = \\frac{1}{|C_1| \\cdot |C_2|} \\sum_{x \\in C_1, y \\in C_2} ||x - y|| $$","metadata":{}},{"id":"33a848e0","cell_type":"code","source":"iris = load_iris()\nX, y = iris.data, iris.target\n\n\nn_pca_components = [2, 3, 4]  \nn_clusters = [2, 3, 4]  \n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nbest_ari = -1\nbest_labels = None\nbest_n_pca = None\nbest_n_clusters = None\nbest_linkage = None\n\nfor n_pca in n_pca_components:\n    for n_cluster in n_clusters:\n        for linkage_method in ['single', 'complete', 'average']:\n                pca = PCA(n_components=n_pca)\n                X_pca = pca.fit_transform(X)\n                clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage_method)\n                labels = clustering.fit_predict(X_pca)\n                ari = adjusted_rand_score(y, labels)\n            \n            if ari > best_ari:\n                best_ari = ari\n                best_labels = labels\n                best_n_pca = n_pca\n                best_n_clusters = n_cluster\n                best_linkage = linkage_method","metadata":{},"outputs":[],"execution_count":null},{"id":"80a6569e","cell_type":"code","source":"# visualizzare i risultati del clustering gerarchico\nplot_clusters_vs_truth(X_pca, best_labels, y_true=y, title_clusters=\"Clustering con Agglomerative\", title_truth=\"Vere Classi\")","metadata":{},"outputs":[],"execution_count":null},{"id":"6441ea48","cell_type":"markdown","source":"### **Esercizio 5.4: Ward Linkage**\nUnisce i cluster la cui fusione determina il minimo incremento della varianza totale intra-cluster (minimizza la somma dei quadrati degli scarti).\n\n#### Vantaggi\n\n* **Cluster compatti e sferici:** Ideale per raggruppamenti di forma regolare.\n\n* **Ottimizzazione della varianza:** Massimizza l'omogeneità interna ai cluster.\n\n#### Svantaggi\n\n* **Elevato costo computazionale:** Poco efficiente su dataset di grandi dimensioni.\n\n* **Ipotesi restrittive:** Assume una struttura di cluster isotropica.\n\n\n#### Formula\n$$ d_{ward}(C_1, C_2) = \\Delta \\text{variance} $$","metadata":{}},{"id":"e417920e","cell_type":"code","source":"iris = load_iris()\nX, y = iris.data, iris.target\n\n\nn_pca_components = [2, 3, 4]  \nn_clusters = [2, 3, 4]  \n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\npca = PCA(n_components=2)\nX_2d = pca.fit_transform(X_scaled)\n\nbest_ari = -1\nbest_config = None\n\n\nfor n_pca in n_pca_components:\n    for n_cluster in n_clusters:\n        # Applica PCA per ridurre a n_pca componenti\n        pca = PCA(n_components=n_pca)\n        X_pca = pca.fit_transform(X_scaled)\n        \n        # Applicazione AgglomerativeClustering con linkage='ward'\n        clustering = AgglomerativeClustering(n_clusters=n_cluster, linkage='ward')\n        labels = clustering.fit_predict(X_pca)\n        \n        # Calcola ARI (Adjusted Rand Index)\n        ari = adjusted_rand_score(y, labels)\n        \n        # Salva la miglior configurazione\n        if ari > best_ari:\n            best_ari = ari\n            best_config = (n_pca, n_cluster)","metadata":{},"outputs":[],"execution_count":null},{"id":"bcae4c8f","cell_type":"code","source":"# visualizza i risultati del clustering gerarchico\nplot_clusters_vs_truth(X_pca, labels, y_true=y, title_clusters=\"Clustering Gerarchico (Ward)\", title_truth=\"Vere Classi\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cd645e9f","cell_type":"markdown","source":"# **Esercizio 6: Testare tutti i linkage method**\n\nInfine dovrete testare contemporaneamente tutti i metod per ottenere tra tutti quello che raggiunge la performance migliore. Dovrete inoltre trovare la configurazione migliore. ","metadata":{}},{"id":"b92b4704","cell_type":"code","source":"iris = load_iris()\nX, y = iris.data, iris.target\n\n\nn_pca_components = [2, 3, 4]  \nn_clusters = [2, 3, 4]  \nlinkage_methods = ['average', 'single', 'complete', 'ward']  \n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nbest_ari = -1\nbest_config = None\nbest_labels = None\nbest_pca = None\nbest_linkage = None\n\nfor pca_components in n_pca_components:\n    for n_cluster in n_clusters:\n        for linkage in linkage_methods:\n            \n            pca = PCA(n_components=pca_components)\n            X_pca = pca.fit_transform(X_scaled)\n            \n            clustering = AgglomerativeClustering(n_clusters=n_cluster, linkage=linkage)\n            labels = clustering.fit_predict(X_pca)\n            \n            ari = adjusted_rand_score(y, labels)\n            \n            if ari > best_ari:\n                best_ari = ari\n                best_config = (pca_components, n_cluster, linkage)\n                best_labels = labels\n                best_pca = pca\n                best_linkage = linkage\n\npca = PCA(n_components=best_config[0])\nX_pca = pca.fit_transform(X_scaled)\n\nclustering = AgglomerativeClustering(n_clusters=best_config[1], linkage=best_linkage)\nlabels = clustering.fit_predict(X_pca)\n\nplot_clusters_vs_truth(X_pca, labels, y_true=y, title_clusters=\"Clustering Gerarchico\", title_truth=\"Vere Classi\")","metadata":{},"outputs":[],"execution_count":null}]}